<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
  /* Design Credits: Jon Barron and Abhishek Kar and Saurabh Gupta*/
  a {
  color: #1772d0;
  text-decoration:none;
  }
  a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
  }
  body,td,th {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 400
  }
  heading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 19px;
    font-weight: 1000
  }
  strong {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 800
  }
  strongred {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    color: 'red' ;
    font-size: 16px
  }
  sectionheading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 22px;
    font-weight: 600
  }
  pageheading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 38px;
    font-weight: 400
  }

  /* added by yc */

  padding {
    padding: 50px 0px;
  }

  </style>
  <link rel="icon" type="image/png" href="img/icon.png">
  <script type="text/javascript" src="js/hidebib.js"></script>
  <title>Tiansheng Wen</title>
  <meta name="Tiansheng Wen's Homepage" http-equiv="Content-Type" content="Tiansheng Wen's Homepage">
  <link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
  <script src="js/scramble.js"></script>
</head>

<body>
<!-- <table width="840" border="0" align="center" border="0" cellspacing="0" cellpadding="20"> -->
<table width="960" border="0" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr><td>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <p align="center">
    <pageheading>Tiansheng Wen (ÊñáÊ∑ªÂú£)</pageheading><br>
    <b>email</b>: neilwen987 _at_ gmail.com
    <font id="email" style="display:inline;">
      <noscript><i>Please enable Javascript to view</i></noscript>
    </font>
  </p>

  <tr>
    <td width="35%" valign="top"><a href="img/profile.png"><img src="img/profile.png" width="100%" style="border-radius:15px"></a>
        <p align=center>
          | <a href="data/CV_3_9.pdf">CV</a> |
          <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=mrdyOyQAAAAJ">Google Scholar</a> |
          <a href="https://github.com/neilwen987">Github</a> |
          <a href="https://x.com/tianshengv111?s=21">X</a>|
          <a href="mailto:neilwen987@gmail.com">Email</a>
<!--          <a href="https://www.linkedin.com/in/yifan-wang-92a593249/">LinkedIn</a> |-->
      </p>
    </td>

    <td width="100%" valign="top" align="justify">
        <p>
         Howdy! Welcome to my home page.
            I am a second-year M.S. student at Xidian University, advised by Prof. <a href="https://web.xidian.edu.cn/bchen/">Bo Chen</a>. Concurrently, I serve as a Research Intern at Stony Brook University, working with Prof. <a href="https://chenyuyou.me/">Chenyu You</a>. Prior to my graduate studies, I received my B.S. degree from Xidian University in 2023.
            <br>
        <br>
        <h2>üî• I am actively seeking for a PhD in 26Fall in US.</h2>
        Please feel free to reach out to me via email if you believe I am a good fit for your research team.
        I welcome the opportunity for further discussion! Please see my
        <a href="data/CV_3_9.pdf">CV</a> for more details.
        </p>
        <h2>üßê Research Interests</h2>
        <div class="research-content">
                    <p>My primary research goal is to develop scalable, <b>reliable and efficient methods</b> for machine learning and generative AI. Currently, I focus on the following key directions:</p>
                      <ol class="research-directions">
                          <li>Bayesian methods for disentangled representations and uncertainty estimation</li>
                          <li>Alignment and safety of Foundation models, including LLMs, VLMs, and diffusion models</li>
                      </ol>

                      <p>In addition, I am also highly interested in:</p>
                      <ul class="other-interests">
                          <li>üìö Memorization in large models</li>
                          <li>üîÑ Self-consuming/self-improving loops</li>
                          <li>ü§ñ Agent learning with Foundation models</li>
                      </ul>

                      <p>If you share the same research interests, feel free to reach out or add my
                          <a href="./img/wechat_qr.png" class="wechat-link">
                              Wechat
<!--                              <img src="./img/wechat_qr.png" alt="WeChat QR Code">-->
                          </a>
                      </p>
                  </div>
        </td>

  </tr>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="8">
  <tr><td>
    <sectionheading>&nbsp;üöÄüöÄ News</sectionheading>
    <ul>
      <li> [03/2025] Code for our paper <a href="https://arxiv.org/abs/2503.01776">CSR</a> has been released,
                and we were invited to publish the model on Hugging Face! ‚öôÔ∏è‚öôÔ∏è </li>
      <li> [02/2025] One paper was accepted by CVPR 2025! üéâüéâ </li>
      <li> [07/2024] Our paper <a href="https://arxiv.org/abs/2407.18589">HICE-Score</a> was accepted by ACM MM 2024! üéâüéâ</li>
    </ul>
  </td></tr>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td><sectionheading>&nbsp;&nbsp;Publications</sectionheading></td></tr>
</table>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15">


  <tr>
    <td width="33%" valign="top" align="center">
        <a href="#">
        <img src="img/csr.png" alt="sym" width="300" height="200" style="border-radius:15px">
        <!-- <video autoplay loop muted playsinline width="300" height="300" style="border-radius:1px">
          <source src="./teaser-thumbnail_sdfusion.mp4" type="video/mp4">
        </video> -->
        </a>
    </td>

    <td width="67%" valign="top">
        <p>
            <a href="https://arxiv.org/abs/2503.01776">

            <heading>Beyond Matryoshka: Revisiting Sparse Coding for Adaptive Representation</heading></a>
            <br>
            <b>Tiansheng Wen*</b>,
              Yifei Wang*,
              Zequn Zeng,
              Zhong Peng,
              Yudi Su,
              Xinyang Liu,
              Bo Chen,
              Hongwei Liu,
              Stefanie Jegelka,
              Chenyu You
            <br>
            <em>arXiv</em>, 2025
        </p>

        <div class="paper" id="csr">
        <a href="javascript:toggleblock('CSR_abs')">abstract</a> |
        <a href="https://arxiv.org/abs/2503.01776">paper</a> |
        <a href="https://github.com/neilwen987/CSR_Adaptive_Rep">code</a> |

        <p align="justify">
            <i id="CSR_abs">
              Many large-scale systems rely on high-quality deep representations (embeddings) to facilitate tasks like retrieval, search, and generative modeling. Matryoshka Representation Learning (MRL) recently emerged as a solution for adaptive embedding lengths, but it requires full model retraining and suffers from noticeable performance degradations at short lengths. In this paper, we show that sparse coding offers a compelling alternative for achieving adaptive representation with minimal overhead and higher fidelity. We propose Contrastive Sparse Representation (CSR), a method that sparsifies pre-trained embeddings into a high-dimensional but selectively activated feature space. By leveraging lightweight autoencoding and task-aware contrastive objectives, CSR preserves semantic quality while allowing flexible, cost-effective inference at different sparsity levels. Extensive experiments on image, text, and multimodal benchmarks demonstrate that CSR consistently outperforms MRL in terms of both accuracy and retrieval speed-often by large margins-while also cutting training time to a fraction of that required by MRL. Our results establish sparse coding as a powerful paradigm for adaptive representation learning in real-world applications where efficiency and fidelity are both paramount. </i>
        </p>

        </div>
    </td>
  </tr>


  <tr>
    <td width="33%" valign="top" align="center">
        <a href="#">
        <img src="img/vqpatch.png" alt="sym" width="300" height="200" style="border-radius:15px">
        <!-- <video autoplay loop muted playsinline width="300" height="300" style="border-radius:1px">
          <source src="./teaser-thumbnail_sdfusion.mp4" type="video/mp4">
        </video> -->
        </a>
    </td>

    <td width="67%" valign="top">
        <p>
            <a href="https://arxiv.org/abs/2401.10278">

            <heading>EEGFormer: Towards Transferable and Interpretable Large-Scale EEG Foundation Model</heading></a>
            <br>
            Yuqi Chen,
            Kan Ren,
            Kaitao Song,
            Yansen Wang,
            <b>Yifan Wang</b>,
            Dongsheng Li,
            Lili Qiu
            <br>
            <em>AAAI</em>, 2024 SSS on Clinical FMs
        </p>

        <div class="paper" id="eegformer">
        <a href="javascript:toggleblock('eegformer_abs')">abstract</a> |
        <a href="https://arxiv.org/abs/2401.10278">paper</a> |

        <p align="justify">
            <i id="eegformer_abs">
              Self-supervised learning has emerged as a highly effective approach in the fields of 
              natural language processing and computer vision. It is also applicable to brain signals such as 
              electroencephalography (EEG) data, given the abundance of available unlabeled data that exist 
              in a wide spectrum of real-world medical applications ranging from seizure detection to wave analysis. 
              The existing works leveraging self-supervised learning on EEG modeling mainly focus on pretraining upon 
              each individual dataset corresponding to a single downstream task, which cannot leverage the power of abundant 
              data, and they may derive sub-optimal solutions with a lack of generalization. Moreover, these methods rely on 
              end-to-end model learning which is not easy for humans to understand. In this paper, 
              we present a novel EEG foundation model, namely EEGFormer, pretrained on large-scale compound EEG data. 
              The pretrained model cannot only learn universal representations on EEG signals with adaptable performance on 
              various downstream tasks but also provide interpretable outcomes of the useful patterns within the data. 
              To validate the effectiveness of our model, we extensively evaluate it on various downstream tasks and assess 
              the performance under different transfer settings. Furthermore, we demonstrate how the learned model exhibits 
              transferable anomaly detection performance and provides valuable interpretability of the acquired patterns 
              via self-supervised learning.
            </i>
        </p>

        </div>
    </td>
  </tr>


  <tr>
    <td width="33%" valign="top" align="center">
        <a href="#">
        <img src="img/veatic.png" alt="sym" width="300" height="200" style="border-radius:15px">
        <!-- <video autoplay loop muted playsinline width="300" height="300" style="border-radius:1px">
          <source src="./teaser-thumbnail_sdfusion.mp4" type="video/mp4">
        </video> -->
        </a>
    </td>

    <td width="67%" valign="top">
        <p>
            <a href="https://veatic.github.io/">

            <heading>VEATIC: Video-based Emotion and Affect Tracking in Context Dataset</heading></a>
            <br>
            Zhihang Ren<sup>*</sup>
            Jefferson Ortega<sup>*</sup>
            <b>Yifan Wang</b><sup>*</sup>
            Zhimin Chen,
            David Whitney,
            Yunhui Guo,
            Stella Yu
            (* Equal contribution)
            <br>
            <em>WACV</em>, 2024
        </p>

        <div class="paper" id="Veatic">
        <a href="https://veatic.github.io/">webpage</a> |
        <a href="javascript:toggleblock('Veatic_abs')">abstract</a> |
        <a href="https://openaccess.thecvf.com/content/WACV2024/papers/Ren_VEATIC_Video-Based_Emotion_and_Affect_Tracking_in_Context_Dataset_WACV_2024_paper.pdf">paper</a> |
        <a href="https://github.com/AlbusPeter/VEATIC">code</a> |

        <p align="justify">
            <i id="Veatic_abs">
              Human affect recognition has been a significant topic in psychophysics and computer vision. 
              However, the currently published datasets have many limitations. For example, most datasets contain frames 
              that contain only information about facial expressions. Due to the limitations of previous datasets, 
              it is very hard to either understand the mechanisms for affect recognition of humans or generalize well on 
              common cases for computer vision models trained on those datasets. In this work, we introduce a brand new 
              large dataset, the Video-based Emotion and Affect Tracking in Context Dataset (VEATIC), that can conquer 
              the limitations of the previous datasets. VEATIC has 124 video clips from Hollywood movies, documentaries, 
              and home videos with continuous valence and arousal ratings of each frame via real-time annotation. 
              Along with the dataset, we propose a new computer vision task to infer the affect of the selected character 
              via both context and character information in each video frame. Additionally, we propose a simple model to 
              benchmark this new computer vision task. We also compare the performance of the pretrained model using our 
              dataset with other similar datasets. Experiments show the competing results of our pretrained model via VEATIC, 
              indicating the generalizability of VEATIC.
            </i>
        </p>

        </div>
    </td>
  </tr>

<table width="100%" align="center" border="0" cellpadding="10">
  <tr>
    <td>
      <sectionheading>&nbsp;&nbsp;Professional Activity</sectionheading>
        <ul>
<!--          <li> Conference Reviewer: MICCAI 2025, CVPR 2025</li>-->
          <li> Journal Reviewer: TNNLS</li>
<!--          <li> Teaching Assistant: CSE 549, IAE 101</li>-->
        </ul>
    </td>
  </tr>
</table>

<!--<table width="100%" align="center" border="0" cellpadding="10">-->
<!--  <tr>-->
<!--    <td>-->
<!--      <sectionheading>&nbsp;&nbsp;Awards</sectionheading>-->
<!--        <ul>-->
<!--          <li> [06/2024] I received the honor of being the <b>Outstanding Graduate</b> in ShanghaiTech. </li>-->
<!--          <li> [12/2023] I received the honor of being 2022-2023 <b>Merit Student</b> in ShanghaiTech. </li>-->
<!--          <li> [07/2023] I received the <b>Undergraduate International Exchange Special Scholarship</b> in ShanghaiTech. </li>-->
<!--          <li> [12/2022] I received the honor of being 2021-2022 <b>Merit Student</b> in ShanghaiTech. </li>-->
<!--        </ul>-->
<!--    </td>-->
<!--  </tr>-->
<!--</table>-->

<script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=200&t=n&d=5Mbudn0FSMUmDjHq2NxUjvoq_vySIg_giud-A4yWPHk"></script>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr><td><br><p align="center"><font size="2">
    Template from this <a href="https://jonbarron.info/">awesome website</a>.
    </font></p></td></tr>
    
</table>
  </td> </tr>
</table>

<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>

<script xml:space="preserve" language="JavaScript">
  hideblock('iclr_25_abs');
</script>

<script xml:space="preserve" language="JavaScript">
  hideblock('eegformer_abs');
</script>

<script xml:space="preserve" language="JavaScript">
  hideblock('Veatic_abs');
</script>

</script>
</body>

</html>