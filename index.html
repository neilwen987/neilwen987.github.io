<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
  /* Design Credits: Jon Barron and Abhishek Kar and Saurabh Gupta*/
  a {
  color: #1772d0;
  text-decoration:none;
  }
  a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
  }
  body,td,th {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 400
  }
  heading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 19px;
    font-weight: 1000
  }
  strong {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 800
  }
  strongred {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    color: 'red' ;
    font-size: 16px
  }
  sectionheading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 22px;
    font-weight: 600
  }
  pageheading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 38px;
    font-weight: 400
  }

  /* added by yc */

  padding {
    padding: 50px 0px;
  }

  </style>
  <link rel="icon" type="image/png" href="img/icon.png">
  <script type="text/javascript" src="js/hidebib.js"></script>
  <title>Tiansheng Wen</title>
  <meta name="Tiansheng Wen's Homepage" http-equiv="Content-Type" content="Tiansheng Wen's Homepage">
  <link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
  <script src="js/scramble.js"></script>
</head>

<body>
<!-- <table width="840" border="0" align="center" border="0" cellspacing="0" cellpadding="20"> -->
<table width="960" border="0" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr><td>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <p align="center">
    <pageheading>Tiansheng Wen (ÊñáÊ∑ªÂú£)</pageheading><br>
    <b>email</b>: neilwen987 _at_ gmail.com
    <font id="email" style="display:inline;">
      <noscript><i>Please enable Javascript to view</i></noscript>
    </font>
  </p>

  <tr>
    <td width="35%" valign="top"><a href="img/profile.png"><img src="img/profile.png" width="100%" style="border-radius:15px"></a>
        <p align=center>
          | <a href="data/CV_3_9.pdf">CV</a> |
          <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=mrdyOyQAAAAJ">Google Scholar</a> |
          <a href="https://github.com/neilwen987">Github</a> |
          <a href="https://x.com/tianshengv111?s=21">X</a>|
          <a href="mailto:neilwen987@gmail.com">Email</a>
<!--          <a href="https://www.linkedin.com/in/yifan-wang-92a593249/">LinkedIn</a> |-->
      </p>
    </td>

    <td width="100%" valign="top" align="justify">
        <p>
         Howdy! Welcome to my home page.
            I am a second-year M.S. student at Xidian University, advised by Prof. <a href="https://web.xidian.edu.cn/bchen/">Bo Chen</a>. Concurrently, I serve as a Research Intern at Stony Brook University, working with Prof. <a href="https://chenyuyou.me/">Chenyu You</a>. Prior to my graduate studies, I received my B.S. degree from Xidian University in 2023.
            <br>
        <br>
        <h2>üî• I am actively seeking for a PhD in 26Fall in US.</h2>
        Please feel free to reach out to me via email if you believe I am a good fit for your research team.
        I welcome the opportunity for further discussion! Please see my
        <a href="data/CV_3_9.pdf">CV</a> for more details.
        </p>
        <h2>üßê Research Interests</h2>
        <div class="research-content">
                    <p>My primary research goal is to develop scalable, <b>reliable and efficient methods</b> for machine learning and generative AI. Currently, I focus on the following key directions:</p>
                      <ol class="research-directions">
                          <li>Bayesian methods for disentangled representations and uncertainty estimation</li>
                          <li>Alignment and safety of Foundation models, including LLMs, VLMs, and diffusion models</li>
                      </ol>

                      <p>In addition, I am also highly interested in:</p>
                      <ul class="other-interests">
                          <li>üìö Memorization in large models</li>
                          <li>üîÑ Self-consuming/self-improving loops</li>
                          <li>ü§ñ Agent learning with Foundation models</li>
                      </ul>

                      <p>If you share the same research interests, feel free to reach out or add my
                          <a href="./img/wechat_qr.png" class="wechat-link">
                              Wechat
<!--                              <img src="./img/wechat_qr.png" alt="WeChat QR Code">-->
                          </a>
                      </p>
                  </div>
        </td>

  </tr>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="8">
  <tr><td>
    <sectionheading>&nbsp;üöÄüöÄ News</sectionheading>
    <ul>
      <li> [03/2025] Code for our paper <a href="https://arxiv.org/abs/2503.01776">CSR</a> has been released,
                and we were invited to publish the model on Hugging Face! ‚öôÔ∏è‚öôÔ∏è </li>
      <li> [02/2025] One paper was accepted by CVPR 2025! üéâüéâ </li>
      <li> [07/2024] Our paper <a href="https://arxiv.org/abs/2407.18589">HICE-Score</a> was accepted by ACM MM 2024! üéâüéâ</li>
    </ul>
  </td></tr>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td><sectionheading>&nbsp;&nbsp;Publications</sectionheading></td></tr>
</table>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15">


  <tr>
    <td width="30%" valign="top" align="center">
        <a href="#">
        <img src="img/csr.png" alt="sym" width="400" height="200" style="border-radius:15px">
        <!-- <video autoplay loop muted playsinline width="300" height="300" style="border-radius:1px">
          <source src="./teaser-thumbnail_sdfusion.mp4" type="video/mp4">
        </video> -->
        </a>
    </td>

    <td width="67%" valign="top">
        <p>
            <a href="https://arxiv.org/abs/2503.01776">

            <heading>Beyond Matryoshka: Revisiting Sparse Coding for Adaptive Representation</heading></a>
            <br>
            <b>Tiansheng Wen<sup>*</sup></b>,
             Yifei Wang<sup>*</sup>,
              Zequn Zeng,
              Zhong Peng,
              Yudi Su,
              Xinyang Liu,
              Bo Chen,
              Hongwei Liu,
              Stefanie Jegelka,
              Chenyu You
            <br>
            <em>arXiv</em>, 2025
        </p>

        <div class="paper" id="csr">
        <a href="javascript:toggleblock('CSR_abs')">abstract</a> |
        <a href="https://arxiv.org/abs/2503.01776">paper</a> |
        <a href="https://github.com/neilwen987/CSR_Adaptive_Rep">code</a> |

        <p align="justify">
            <i id="CSR_abs">
              Many large-scale systems rely on high-quality deep representations (embeddings) to facilitate tasks like retrieval,
                search, and generative modeling.
                Matryoshka Representation Learning (MRL) recently emerged as a solution for adaptive embedding lengths, but it
                requires full model retraining and suffers from noticeable performance degradations at short lengths.
                In this paper, we show that sparse coding offers a compelling alternative for achieving adaptive representation
                with minimal overhead and higher fidelity.
                We propose Contrastive Sparse Representation (CSR), a method that sparsifies pre-trained embeddings into a
                high-dimensional but selectively activated feature space.
                By leveraging lightweight autoencoding and task-aware contrastive objectives, CSR preserves semantic
                quality while allowing flexible, cost-effective inference at different sparsity levels.
                Extensive experiments on image, text, and multimodal benchmarks demonstrate that CSR consistently
                outperforms MRL in terms of both accuracy and retrieval speed-often by large margins-while also cutting
                training time to a fraction of that required by MRL.
                Our results establish sparse coding as a powerful paradigm for adaptive representation learning in real-world
                applications where efficiency and fidelity are both paramount.
            </i>
        </p>

        </div>
    </td>
  </tr>


  <tr>
    <td width="33%" valign="top" align="center">
        <a href="#">
        <img src="img/vqpatch.png" alt="sym" width="300" height="200" style="border-radius:15px">
        <!-- <video autoplay loop muted playsinline width="300" height="300" style="border-radius:1px">
          <source src="./teaser-thumbnail_sdfusion.mp4" type="video/mp4">
        </video> -->
        </a>
    </td>

    <td width="67%" valign="top">
        <p>
            <a href="https://arxiv.org/abs/2407.21740">

            <heading>Contrastive Factor Analysis</heading></a>
            <br>
            Zhibin Duan<sup>*</sup>,
            <b>Tiansheng Wen<sup>*</sup></b>,
            Yifei Wang,
            Chen Zhu,
            Bo Chen,
            Mingyuan Zhou
            <br>
            <em>arXiv</em>, 2024
        </p>

        <div class="paper" id="eegformer">
        <a href="javascript:toggleblock('cfa_abs')">abstract</a> |
        <a href="https://arxiv.org/abs/2401.10278">paper</a> |

        <p align="justify">
            <i id="cfa_abs">
              Factor analysis, often regarded as a Bayesian variant of matrix factorization,
                offers superior capabilities in capturing uncertainty, modeling complex dependencies,
                and ensuring robustness. As the deep learning era arrives, factor analysis is receiving
                less and less attention due to their limited expressive ability. On the contrary,
                contrastive learning has emerged as a potent technique with demonstrated efficacy
                in unsupervised representational learning. While the two methods are different paradigms,
                recent theoretical analysis has revealed the mathematical equivalence between contrastive
                learning and matrix factorization, providing a potential possibility for factor analysis
                combined with contrastive learning. Motivated by the interconnectedness of contrastive
                learning, matrix factorization, and factor analysis, this paper introduces a novel
                Contrastive Factor Analysis framework, aiming to leverage factor analysis's advantageous
                properties within the realm of contrastive learning. To further leverage the interpretability
                properties of non-negative factor analysis, which can learn disentangled representations,
                contrastive factor analysis is extended to a non-negative version. Finally, extensive
                experimental validation showcases the efficacy of the proposed contrastive (non-negative)
                factor analysis methodology across multiple key properties, including expressiveness,
                robustness, interpretability, and accurate uncertainty estimation.
            </i>
        </p>

        </div>
    </td>
  </tr>


  <tr>
    <td width="33%" valign="top" align="center">
        <a href="#">
        <img src="img/hice.png.png" alt="sym" width="300" height="200" style="border-radius:15px">
        <!-- <video autoplay loop muted playsinline width="300" height="300" style="border-radius:1px">
          <source src="./teaser-thumbnail_sdfusion.mp4" type="video/mp4">
        </video> -->
        </a>
    </td>

    <td width="67%" valign="top">
        <p>
            <a href="https://dl.acm.org/doi/abs/10.1145/3664647.3681358">

            <heading>HICEScore: A Hierarchical Metric for Image Captioning Evaluation</heading></a>
            <br>
            Zequn Zeng,
            Jianqiao Sun,
            Hao Zhang,
            <b>Tiansheng Wen</b>,
            Yudi Su,
            Yan Xie,
            Zhengjue Wang,
            Bo Chen
            <br>
            <em>ACM MM</em>, 2024
        </p>

        <div class="paper" id="Veatic">
<!--        <a href="https://veatic.github.io/">webpage</a> |-->
        <a href="javascript:toggleblock('hice_abs')">abstract</a> |
        <a href="https://dl.acm.org/doi/abs/10.1145/3664647.3681358">paper</a> |
        <a href="https://github.com/joeyz0z/HICE">code</a> |

        <p align="justify">
            <i id="hice_abs">
Image captioning evaluation metrics can be divided into two categories, reference-based metrics and reference-free metrics. However, reference-based approaches may struggle to evaluate descriptive captions with abundant visual details produced by advanced multimodal large language models, due to their heavy reliance on limited human-annotated references. In contrast, previous reference-free metrics have been proven effective via CLIP cross-modality similarity. Nonetheless, CLIP-based metrics, constrained by their solution of global image-text compatibility, often have a deficiency in detecting local textual hallucinations and are insensitive to small visual objects. Besides, their single-scale designs are unable to provide an interpretable evaluation process such as pinpointing the position of caption mistakes and identifying visual regions that have not been described. To move forward, we propose a novel reference-free metric for image captioning evaluation, dubbed Hierarchical Image Captioning Evaluation Score (HICE-S). By detecting local visual regions and textual phrases, HICE-S builds an interpretable hierarchical scoring mechanism, breaking through the barriers of the single-scale structure of existing reference-free metrics. Comprehensive experiments indicate that our proposed metric achieves the SOTA performance on several benchmarks, outperforming existing reference-free metrics like CLIP-S and PAC-S, and reference-based metrics like METEOR and CIDEr. Moreover, several case studies reveal that the assessment process of HICE-S on detailed captions closely resembles interpretable human judgments.
                Our code is available at https://github.com/joeyz0z/HICE.
            </i>
        </p>

        </div>
    </td>
  </tr>

<table width="100%" align="center" border="0" cellpadding="10">
  <tr>
    <td>
      <sectionheading>&nbsp;&nbsp;Professional Activity</sectionheading>
        <ul>
<!--          <li> Conference Reviewer: MICCAI 2025, CVPR 2025</li>-->
          <li> Journal Reviewer: TNNLS</li>
<!--          <li> Teaching Assistant: CSE 549, IAE 101</li>-->
        </ul>
    </td>
  </tr>
</table>

<!--<table width="100%" align="center" border="0" cellpadding="10">-->
<!--  <tr>-->
<!--    <td>-->
<!--      <sectionheading>&nbsp;&nbsp;Awards</sectionheading>-->
<!--        <ul>-->
<!--          <li> [06/2024] I received the honor of being the <b>Outstanding Graduate</b> in ShanghaiTech. </li>-->
<!--          <li> [12/2023] I received the honor of being 2022-2023 <b>Merit Student</b> in ShanghaiTech. </li>-->
<!--          <li> [07/2023] I received the <b>Undergraduate International Exchange Special Scholarship</b> in ShanghaiTech. </li>-->
<!--          <li> [12/2022] I received the honor of being 2021-2022 <b>Merit Student</b> in ShanghaiTech. </li>-->
<!--        </ul>-->
<!--    </td>-->
<!--  </tr>-->
<!--</table>-->

<script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=200&t=n&d=5Mbudn0FSMUmDjHq2NxUjvoq_vySIg_giud-A4yWPHk"></script>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr><td><br><p align="center"><font size="2">
    Template from this <a href="https://jonbarron.info/">awesome website</a>.
    </font></p></td></tr>
    
</table>
  </td> </tr>
</table>

<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>

<script xml:space="preserve" language="JavaScript">
  hideblock('iclr_25_abs');
</script>

<script xml:space="preserve" language="JavaScript">
  hideblock('eegformer_abs');
</script>

<script xml:space="preserve" language="JavaScript">
  hideblock('Veatic_abs');
</script>

</script>
</body>

</html>